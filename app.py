import subprocess
import sys
import os
import shutil
import gradio as gr
import modelscope_studio as mgr
import uvicorn
from fastapi import FastAPI
from modelscope import snapshot_download
import warnings
import time
import socket
import nltk

# åˆ›ç©ºé—´éƒ¨ç½²éœ€è¦
import ssl
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

warnings.filterwarnings("ignore")

# os.environ["DASHSCOPE_API_KEY"] = "INPUT YOUR API KEY HERE"
os.environ["is_half"] = "True"

# å®‰è£…musetalkä¾èµ–
os.system('mim install mmengine')
os.system('mim install "mmcv==2.1.0"')
os.system('mim install "mmdet==3.2.0"')
# os.system('mim install "mmpose==1.2.0"') # for torch 2.1.2
os.system('mim install "mmpose==1.3.2"') # for torch 2.3.0
shutil.rmtree('./workspaces/results', ignore_errors= True)

# GLM-4-Voice é…ç½®
sys.path.insert(0, "./src/GLM_4_Voice")
sys.path.insert(0, "./src/GLM_4_Voice/cosyvoice")
sys.path.insert(0, "./src/GLM_4_Voice//third_party/Matcha-TTS")

snapshot_download('ZhipuAI/glm-4-voice-tokenizer',cache_dir='./weights')
snapshot_download('ZhipuAI/glm-4-voice-decoder',cache_dir='./weights')
snapshot_download('ZhipuAI/glm-4-voice-9b',cache_dir='./weights')

from src.pipeline_llm import llm_pipeline
from src.pipeline_mllm import mllm_pipeline


def is_port_open(host, port):
    """Check if a port is open on the given host."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        try:
            s.connect((host, port))
            return True
        except (ConnectionRefusedError, socket.timeout):
            return False


def wait_for_port(host, port, timeout=1800):
    """Wait for a port to open within a specified timeout."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        if is_port_open(host, port):
            print(f"Port {port} is open on {host}.")
            return True
        time.sleep(1)
    print(f"Timeout: Port {port} is not open on {host} after {timeout} seconds.")
    return False


def create_gradio():
    with gr.Blocks() as cascade_demo: 

        gr.Markdown(
            """
            <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
            Chat with Digital Human (ASR-LLM-TTS-THG)
            </div>  
            <div style="text-align: center;">
               <a href="https://github.com/Henry-23/VideoChat"> GitHub </a> |
               <a href="https://mp.weixin.qq.com/s/jpoB8O2IyjhXeAWNWnAj7A"> ç¤¾åŒºæ–‡ç«  </a>
            </div>

            """
        )
        with gr.Row():
            with gr.Column(scale = 2):
                user_chatbot = mgr.Chatbot(
                    label = "Chat History ğŸ’¬",
                    value = [[None, {"text":"æ‚¨å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨ï¼Ÿæ‚¨å¯ä»¥åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ç‚¹å‡»éº¦å…‹é£å½•åˆ¶éŸ³é¢‘æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬ä¸æˆ‘èŠå¤©ã€‚"}],],
                    avatar_images=[
                        {"avatar": os.path.abspath("data/icon/user.png")},
                        {"avatar": os.path.abspath("data/icon/qwen.png")},
                    ],
                    height= 500,
                    ) 

                with gr.Row():
                    avatar_name = gr.Dropdown(label = "æ•°å­—äººå½¢è±¡", choices = ["Avatar1 (é€šä¹‰ä¸‡ç›¸)", "Avatar2 (é€šä¹‰ä¸‡ç›¸)", "Avatar3 (MuseV)"], value = "Avatar1 (é€šä¹‰ä¸‡ç›¸)")
                    chat_mode = gr.Dropdown(label = "å¯¹è¯æ¨¡å¼", choices = ["å•è½®å¯¹è¯ (ä¸€æ¬¡æ€§å›ç­”é—®é¢˜)", "äº’åŠ¨å¯¹è¯ (åˆ†å¤šæ¬¡å›ç­”é—®é¢˜)"], value = "å•è½®å¯¹è¯ (ä¸€æ¬¡æ€§å›ç­”é—®é¢˜)")
                    chunk_size = gr.Slider(label = "æ¯æ¬¡å¤„ç†çš„å¥å­æœ€çŸ­é•¿åº¦", minimum = 0, maximum = 30, value = 5, step = 1) 
                    tts_module = gr.Dropdown(label = "TTSé€‰å‹", choices = ["GPT-SoVits", "CosyVoice"], value = "CosyVoice")
                    avatar_voice = gr.Dropdown(label = "TTSéŸ³è‰²", choices = ["longxiaochun (CosyVoice)", "longwan (CosyVoice)", "longcheng (CosyVoice)", "longhua (CosyVoice)", "å°‘å¥³ (GPT-SoVits)", "å¥³æ€§ (GPT-SoVits)", "é’å¹´ (GPT-SoVits)", "ç”·æ€§ (GPT-SoVits)"], value="longwan (CosyVoice)")
                    
                user_input = mgr.MultimodalInput(sources=["microphone"])

            with gr.Column(scale = 1):
                video_stream = gr.Video(label="Video Stream ğŸ¬ (åŸºäºGradio 5ï¼Œå¯èƒ½å¡é¡¿ï¼Œå¯å‚è€ƒå·¦ä¾§å¯¹è¯æ¡†ç”Ÿæˆçš„å®Œæ•´è§†é¢‘ã€‚)", streaming=True, height = 500, scale = 1)  
                user_input_audio = gr.Audio(label="éŸ³è‰²å…‹éš†(å¯é€‰é¡¹ï¼Œè¾“å…¥éŸ³é¢‘æ§åˆ¶åœ¨3-10sã€‚å¦‚æœä¸éœ€è¦éŸ³è‰²å…‹éš†ï¼Œè¯·æ¸…ç©ºã€‚)", sources = ["microphone", "upload"],type = "filepath")
                stop_button = gr.Button(value="åœæ­¢ç”Ÿæˆ")

        # Use State to store user chat history
        user_messages = gr.State([{'role': 'system', 'content': None}])
        user_processing_flag = gr.State(False)
        lifecycle = mgr.Lifecycle()

        # voice clone
        user_input_audio.stop_recording(llm_pipeline.load_voice,
            inputs = [avatar_voice, tts_module, user_input_audio],
            outputs = [user_input])

        # loading TTS Voice
        avatar_voice.change(llm_pipeline.load_voice, 
            inputs=[avatar_voice, tts_module, user_input_audio], 
            outputs=[user_input]
            )
        lifecycle.mount(llm_pipeline.load_voice,
            inputs=[avatar_voice, tts_module, user_input_audio],
            outputs=[user_input]
        )

        # Submit
        user_input.submit(llm_pipeline.run_pipeline,
            inputs=[user_input, user_messages, chunk_size, avatar_name, tts_module, chat_mode, user_input_audio], 
            outputs=[user_messages]
            )
        user_input.submit(llm_pipeline.yield_results, 
            inputs=[user_input, user_chatbot, user_processing_flag],
            outputs = [user_input, user_chatbot, video_stream, user_processing_flag]
            )

        # refresh
        lifecycle.unmount(llm_pipeline.stop_pipeline, 
            inputs = user_processing_flag, 
            outputs = user_processing_flag
            )

        # stop
        stop_button.click(llm_pipeline.stop_pipeline, 
            inputs = user_processing_flag, 
            outputs = user_processing_flag
            )
        
    with gr.Blocks() as mllm_demo:
          
        gr.Markdown(
            """
            <div style="text-align: center; font-size: 32px; font-weight: bold; margin-bottom: 20px;">
            Chat with Digital Human (GLM-4-Voice - THG)
            </div>  
            <div style="text-align: center;">
               <a href="https://github.com/Henry-23/VideoChat"> GitHub </a> |
               <a href="https://mp.weixin.qq.com/s/jpoB8O2IyjhXeAWNWnAj7A"> ç¤¾åŒºæ–‡ç«  </a>
            </div>
            """
        )
        with gr.Row():
            with gr.Column(scale = 2):
                user_chatbot = mgr.Chatbot(
                    label = "Chat History ğŸ’¬",
                    value = [[None, {"text":"æ‚¨å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨ï¼Ÿæ‚¨å¯ä»¥åœ¨ä¸‹æ–¹çš„è¾“å…¥æ¡†ç‚¹å‡»éº¦å…‹é£å½•åˆ¶éŸ³é¢‘æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬ä¸æˆ‘èŠå¤©ã€‚\næˆ‘ä½¿ç”¨äº†æ™ºè°±AIå¼€æºçš„ç«¯åˆ°ç«¯è¯­éŸ³æ¨¡å‹GLM-4-Voiceï¼Œæ‚¨å¯ä»¥é€šè¿‡ç®€å•çš„æŒ‡ä»¤æ§åˆ¶æƒ…ç»ªã€ç”Ÿæˆæ–¹è¨€ç­‰ï¼Œä¾‹å¦‚ï¼š\nâ€œç”¨è½»æŸ”çš„å£°éŸ³å¼•å¯¼æˆ‘æ”¾æ¾ã€‚â€\nâ€œç”¨ä¸œåŒ—è¯ä»‹ç»ä¸€ä¸‹å†¬å¤©æœ‰å¤šå†·ã€‚â€\nâ€œç”¨åŒ—äº¬è¯å¿µä¸€å¥ç»•å£ä»¤ã€‚â€\n"}],],
                    avatar_images=[
                        {"avatar": os.path.abspath("data/icon/user.png")},
                        {"avatar": os.path.abspath("data/icon/qwen.png")},
                    ],
                    height= 500,
                ) 

                with gr.Row():
                    avatar_name = gr.Dropdown(label = "æ•°å­—äººå½¢è±¡", choices = ["Avatar1 (é€šä¹‰ä¸‡ç›¸)", "Avatar2 (é€šä¹‰ä¸‡ç›¸)", "Avatar3 (MuseV)"], value = "Avatar1 (é€šä¹‰ä¸‡ç›¸)")

                  
                user_input = mgr.MultimodalInput(sources=["microphone"])

            with gr.Column(scale = 1):
                video_stream = gr.Video(label="Video Stream ğŸ¬ (åŸºäºGradio 5ï¼Œå¯èƒ½å¡é¡¿ï¼Œå¯å‚è€ƒå·¦ä¾§å¯¹è¯æ¡†ç”Ÿæˆçš„å®Œæ•´è§†é¢‘ã€‚)", streaming=True, height = 500, scale = 1)  

        user_messages = gr.State("") #ä¿å­˜ä¸Šä¸€è½®ä¼šè¯çš„token

        # GLM mode
        user_input.submit(
            mllm_pipeline.run_pipeline,
            inputs=[user_input, user_messages, avatar_name], 
            outputs=[user_messages]
            )
        user_input.submit(
            mllm_pipeline.yield_results, 
            inputs=[user_input, user_chatbot],
            outputs = [user_input, user_chatbot, video_stream]
            )
        

    return gr.TabbedInterface([cascade_demo, mllm_demo], ['ASR-LLM-TTS-THG', 'MLLM(GLM-4-Voice)-THG']).queue()

if __name__ == "__main__":
    # å¯åŠ¨ model_server
    model_server_process = subprocess.Popen(
        ['python', 'server.py'],
        cwd="./",
    )

    # ç­‰å¾… model_server å¯åŠ¨å¹¶æ‰“å¼€ç«¯å£
    if wait_for_port('localhost', 10000):
        
        try:
            # warm up 
            mllm_pipeline.mllm.warm_up()
            # å¯åŠ¨ gradio demo
            print("Starting FastAPI with Gradio...")
            app = FastAPI()
            gradio_app = create_gradio()
            app = gr.mount_gradio_app(app, gradio_app, path='/')
            uvicorn.run(app, port=7860)

            # ç­‰å¾… model_server è¿›ç¨‹ç»“æŸ
            model_server_process.wait()

        except KeyboardInterrupt:
            print("Terminating processes...")
            model_server_process.terminate()
    else:
        print("Failed to start model_server, terminating...")
        model_server_process.terminate()